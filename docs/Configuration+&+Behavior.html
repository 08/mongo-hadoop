<!DOCTYPE html SYSTEM "about:legacy-compat">
<html manifest="pamflet.manifest">
      <head>
        <title>MongoDB+Hadoop Connector — Configuration &amp; Behavior</title>
        <link type="text/css" media="screen, projection" rel="stylesheet" href="css/blueprint/screen.css"></link>
        <link type="text/css" media="screen and (min-device-width: 800px), projection" rel="stylesheet" href="css/blueprint/grid.css"></link>
        <link type="text/css" media="print" rel="stylesheet" href="css/blueprint/print.css"></link> 
        <!--[if lt IE 8]>
          <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection"/>
        <![endif]-->
        <link type="text/css" media="screen, projection" rel="stylesheet" href="css/pamflet.css"></link>
        <link type="text/css" media="print" rel="stylesheet" href="css/pamflet-print.css"></link>
        <link type="text/css" media="screen and (min-device-width: 800px), projection" rel="stylesheet" href="css/pamflet-grid.css"></link>
        
        <script src="js/jquery-1.6.2.min.js"></script>
        <script src="js/jquery.collapse.js"></script>
        <script src="js/pamflet.js"></script>
        
        <meta charset="utf-8"></meta>
        <meta name="viewport" content="width=device-width, initial-scale=1"></meta>
      </head>
      <body>
        <a class="page prev nav" href="Building+the+Adapter.html">
            <span class="space">&nbsp;</span>
            <span class="flip">❧</span>
          </a><a class="page next nav" href="Hadoop+Streaming+Support.html">
            <span class="space">&nbsp;</span>
            <span>❧</span>
          </a>
        <div class="container">
          <div class="span-16 prepend-1 append-1">
            <div class="top nav span-16 title">
              <span>MongoDB+Hadoop Connector</span> — Configuration &amp; Behavior
            </div>
          </div>
          <div class="span-16 prepend-1 append-1 contents">
            <h1 id="Configuration+%26+Behavior">Configuration &amp; Behavior</h1><h2 id="Hadoop+MapReduce">Hadoop MapReduce</h2><p>This package provides working <em>Input</em> and <em>Output</em> adapters for MongoDB.  You may
configure these adapters with XML or programatically. See the
WordCount examples for demonstrations of both approaches.  You can
specify a query, fields and sort specs in the XML config as JSON or
programatically as a DBObject.
</p><h3 id="Splitting+up+MongoDB+Source+Data+for+the+InputFormat">Splitting up MongoDB Source Data for the InputFormat</h3><p>The MongoDB Hadoop Adapter makes it possible to create multiple
<em>InputSplits</em> on source data originating from MongoDB to
optimize/paralellize input processing for Mappers.
</p><p>If <code>mongo.input.split.create_input_splits</code> is <strong>false</strong> (it defaults
to <strong>true</strong>) then MongoHadoop will use <strong>no</strong> splits. Hadoop will
treat the entire collection in a single, giant, <em>Input</em>.  This is
primarily intended for debugging purposes.
</p><p>When true, as by default, the following possible behaviors exist:
</p><ol><li><p>For unsharded the source collections, MongoHadoop follows the
</p><p> “unsharded split” path. (See below.)
</p></li><li><p>For sharded source collections:
</p><ul><li>If <code>mongo.input.split.read_shard_chunks</code> is <strong>true</strong> (defaults <strong>true</strong>) then we pull the chunk specs from the
configuration server, and turn each shard chunk into an <em>Input Split</em>.  Basically, this means the mongodb sharding system does 99% of the preconfig work for us and is a good thing‚Ñ¢
</li></ul><ul><li>If <code>read_shard_chunks</code> is <strong>false</strong> and <code>mongo.input.split.read_from_shards</code> is <strong>true</strong> (it defaults to <strong>false</strong>) then we connect to the <code>mongod</code> or replica set for each shard individually and each shard becomes an input split. The entire content of the collection on the shard is one split. Only use this configuration in rare situations.
</li></ul><ul><li>If <code>read_shard_chunks</code> is <strong>true</strong> and <code>mongo.input.split.read_from_shards</code> is <strong>true</strong> (it defaults to <strong>false</strong>) MongoHadoop reads the chunk boundaries from the config server but then reads data directly from the shards without using the <code>mongos</code>.  While this may seem like a good idea, it can cause erratic behavior if MongoDB balances chunks during a Hadoop job. This is not a recommended configuration for write heavy applications but may provide effective parallelism in read heavy apps.
</li></ul><ul><li>If both <code>create_input_splits</code> and <code>read_from_shards</code> are <strong>false</strong> disabled then we pretend there is no sharding and use the “unsharded split” path. When <code>read_shard_chunks</code> is <strong>false</strong> MongoHadoop reads everything through mongos as a single split.
</li></ul></li></ol><h3 id="%E2%80%9CUnsharded+Splits%E2%80%9C">“Unsharded Splits“</h3><p>“Unsharded Splits” refers to the system that MongoHadoop uses to
calculate new splits. You may use “Unsharded splits” with sharded
MongoDB options.
</p><p>This is only used:
</p><ul><li>for unsharded collections when
<code>mongo.input.split.create_input_splits</code> is <strong>true</strong>.
</li><li>for sharded collections when
<code>mongo.input.split.create_input_splits</code> is <strong>true</strong> <em>and</em>
<code>read_shard_chunks</code> is <strong>false</strong>.
</li></ul><p>In these cases, MongoHadoop generates multiple InputSplits. Users
have control over two factors in this system.
</p><ul><li><code>mongo.input.split_size</code> - Controls the maximum number of megabytes
of each split.  The current default is 8, based on assumptions
prior experience with Hadoop. MongoDB’s default of 64 megabytes
may be a bit too large for most deployments.
</li><li><code>mongo.input.split.split_key_pattern</code> - Is a MongoDB key pattern
that follows <a  href="http://www.mongodb.org/display/DOCS/Sharding+Introduction#ShardingIntroduction-ShardKeys">the same rules as shard key selection</a>.
This key pattern has some requires, (i.e. must have an index,
unique, and present in all documents.) MongoHadoop uses this key to
determine split points. It defaults to <code>{ _id: 1 }</code> but you may find
that its more ideal  to optimize the mapper distribution by
configuring this value.
</li></ul><p>For all three paths, you may specify a custom query filter for the
input data. <em>mongo.input.query</em> represents a JSON document containing
a MongoDB query.  This will be properly combined with the index
filtering on input splits allowing you to MapReduce a subset of your
data but still get efficient splitting.
</p><div class="tocwrapper show">
      <a class="tochead nav" style="display: none" href="#toc">❦</a>
      <a name="toc"></a>
      <h4 class="toctitle">Contents</h4>
      <div class="tocbody">
      <div><a href="MongoDB%2BHadoop+Connector.html">MongoDB+Hadoop Connector</a></div><ol class="toc"> <li><div><a href="Frequently+Asked+Questions.html">Frequently Asked Questions</a></div></li><li><div><a href="Getting+Started.html">Getting Started</a></div><ol class="toc"> <li><div><a href="Building+the+Adapter.html">Building the Adapter</a></div></li><li><div class="current">Configuration &amp; Behavior</div></li> </ol></li><li><div><a href="Hadoop+Streaming+Support.html">Hadoop Streaming Support</a></div><ol class="toc"> <li><div><a href="Building+Hadoop+Streaming+Support.html">Building Hadoop Streaming Support</a></div></li> </ol></li><li class="generated"><div><a href="Contents+in+Depth.html">Contents in Depth</a></div></li><li class="generated"><div><a href="Combined+Pages.html">Combined Pages</a></div></li> </ol></div></div>
          </div>
        </div>
        <a class="fork nav" href="http://github.com/mongodb/mongo-hadoop"><img alt="Fork me on GitHub" src="img/fork.png"></img></a>
      </body>
    </html>